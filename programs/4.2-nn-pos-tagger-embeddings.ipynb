{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-Speech Tagging using Feedforward Networks and Embeddings\n",
    "Author: Pierre Nugues\n",
    "\n",
    "A part-of-speech tagger using feed-forward networks and GloVe embeddings and trained on a corpus following the Universal Dependencies format. Here we use the English Web Treebank:\n",
    "https://github.com/UniversalDependencies/UD_English-EWT/tree/master."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import time\n",
    "from keras import models, layers\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.models import load_model\n",
    "import math\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZER = 'rmsprop'\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 4\n",
    "MINI_CORPUS = False\n",
    "EMBEDDING_DIM = 100\n",
    "W_SIZE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Embeddings\n",
    "We will use GloVe embeddings and load them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(file):\n",
    "    \"\"\"\n",
    "    Return the embeddings in the from of a dictionary\n",
    "    :param file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    file = file\n",
    "    embeddings = {}\n",
    "    glove = open(file)\n",
    "    for line in glove:\n",
    "        values = line.strip().split()\n",
    "        word = values[0]\n",
    "        vector = np.array(values[1:], dtype='float32')\n",
    "        embeddings[word] = vector\n",
    "    glove.close()\n",
    "    embeddings_dict = embeddings\n",
    "    embedded_words = sorted(list(embeddings_dict.keys()))\n",
    "    return embeddings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file = '/Users/pierre/Documents/Cours/EDAN20/corpus/glove.6B.100d.txt'\n",
    "embeddings_dict = load(embedding_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict['table']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ud_en_ewt():\n",
    "\n",
    "    train_file = '../datasets/ud_en/en_ewt-ud-train.conllu'\n",
    "    dev_file = '../datasets/ud_en/en_ewt-ud-dev.conllu'\n",
    "    test_file = '../datasets/ud_en/en_ewt-ud-test.conllu'\n",
    "    column_names = ['ID', 'FORM', 'LEMMA', 'UPOS', 'XPOS', \n",
    "                    'FEATS', 'HEAD', 'DEPREL', 'HEAD', 'DEPS', 'MISC']\n",
    "    column_names = list(map(str.lower, column_names))\n",
    "    train_sentences = open(train_file).read().strip()\n",
    "    dev_sentences = open(dev_file).read().strip()\n",
    "    test_sentences = open(test_file).read().strip()\n",
    "    # test2_sentences = open(test2_file).read().strip()\n",
    "    return train_sentences, dev_sentences, test_sentences, column_names\n",
    "\n",
    "def load_conll2009_pos():\n",
    "    train_file = '/Users/pierre/Documents/Cours/EDAN20/corpus/conll2009/en/CoNLL2009-ST-English-train-pos.txt'\n",
    "    dev_file = '/Users/pierre/Documents/Cours/EDAN20/corpus/conll2009/en/CoNLL2009-ST-English-development-pos.txt'\n",
    "    test_file = '/Users/pierre/Documents/Cours/EDAN20/corpus/conll2009/en/CoNLL2009-ST-test-words-pos.txt'\n",
    "    # test2_file = 'simple_pos_test.txt'\n",
    "\n",
    "    column_names = ['id', 'form', 'lemma', 'plemma', 'pos', 'ppos']\n",
    "\n",
    "    train_sentences = open(train_file).read().strip()\n",
    "    dev_sentences = open(dev_file).read().strip()\n",
    "    test_sentences = open(test_file).read().strip()\n",
    "    # test2_sentences = open(test2_file).read().strip()\n",
    "    return train_sentences, dev_sentences, test_sentences, column_names\n",
    "\n",
    "# train_sentences, dev_sentences, test_sentences, column_names = \\\n",
    "# load_conll2009_pos()\n",
    "train_sentences, dev_sentences, test_sentences, column_names =\\\n",
    "load_ud_en_ewt()\n",
    "train_sentences[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the Corpus in a Dictionary\n",
    "We follow the fit-transform pattern of sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "class Token(dict):\n",
    "    pass\n",
    "\n",
    "class CoNLLDictorizer:\n",
    "\n",
    "    def __init__(self, column_names, sent_sep='\\n\\n', col_sep=' +'):\n",
    "        self.column_names = column_names\n",
    "        self.sent_sep = sent_sep\n",
    "        self.col_sep = col_sep\n",
    "\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, corpus):\n",
    "        corpus = corpus.strip()\n",
    "        sentences = re.split(self.sent_sep, corpus)\n",
    "        return list(map(self._split_in_words, sentences))\n",
    "\n",
    "    def fit_transform(self, corpus):\n",
    "        return self.transform(corpus)\n",
    "\n",
    "    def _split_in_words(self, sentence):\n",
    "        rows = re.split('\\n', sentence)\n",
    "        rows = [row for row in rows if row[0] != '#']\n",
    "        return [Token(dict(zip(self.column_names,\n",
    "                               re.split(self.col_sep, row))))\n",
    "                for row in rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll_dict = CoNLLDictorizer(column_names, col_sep='\\t')\n",
    "train_dict = conll_dict.transform(train_sentences)\n",
    "if MINI_CORPUS:\n",
    "    train_dict = train_dict[:len(train_dict) // 15]\n",
    "test_dict = conll_dict.transform(test_sentences)\n",
    "print('First sentence, train:', train_dict[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the Context and Dictorizing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextDictorizer():\n",
    "    \"\"\"\n",
    "    Extract contexts of words in a sequence\n",
    "    Contexts are of w_size to the left and to the right\n",
    "    Builds an X matrix in the form of a dictionary\n",
    "    and possibly extracts the output, y, if not in the test step\n",
    "    If the test_step is True, returns y = []\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input='form', output='upos', w_size=2, tolower=True):\n",
    "        self.BOS_symbol = '__BOS__'\n",
    "        self.EOS_symbol = '__EOS__'\n",
    "        self.input = input\n",
    "        self.output = output\n",
    "        self.w_size = w_size\n",
    "        self.tolower = tolower\n",
    "        # This was not correct as the names were not sorted\n",
    "        # self.feature_names = [input + '_' + str(i)\n",
    "        #                     for i in range(-w_size, w_size + 1)]\n",
    "        # To be sure the names are ordered\n",
    "        zeros = math.ceil(math.log10(2 * w_size + 1))\n",
    "        self.feature_names = [input + '_' + str(i).zfill(zeros) for \n",
    "                              i in range(2 * w_size + 1)]\n",
    "\n",
    "    def fit(self, sentences):\n",
    "        \"\"\"\n",
    "        Build the padding rows\n",
    "        :param sentences:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.column_names = sentences[0][0].keys()\n",
    "        start = [self.BOS_symbol] * len(self.column_names)\n",
    "        end = [self.EOS_symbol] * len(self.column_names)\n",
    "        start_token = Token(dict(zip(self.column_names, start)))\n",
    "        end_token = Token(dict(zip(self.column_names, end)))\n",
    "        self.start_rows = [start_token] * self.w_size\n",
    "        self.end_rows = [end_token] * self.w_size\n",
    "\n",
    "    def transform(self, sentences, training_step=True):\n",
    "        X_corpus = []\n",
    "        y_corpus = []\n",
    "        for sentence in sentences:\n",
    "            X, y = self._transform_sentence(sentence, training_step)\n",
    "            X_corpus += X\n",
    "            if training_step:\n",
    "                y_corpus += y\n",
    "        return X_corpus, y_corpus\n",
    "\n",
    "    def fit_transform(self, sentences):\n",
    "        self.fit(sentences)\n",
    "        return self.transform(sentences)\n",
    "\n",
    "    def _transform_sentence(self, sentence, training_step=True):\n",
    "        # We extract y\n",
    "        if training_step:\n",
    "            y = [row[self.output] for row in sentence]\n",
    "        else:\n",
    "            y = None\n",
    "\n",
    "        # We pad the sentence\n",
    "        sentence = self.start_rows + sentence + self.end_rows\n",
    "\n",
    "        # We extract the features\n",
    "        X = list()\n",
    "        for i in range(len(sentence) - 2 * self.w_size):\n",
    "            # x is a row of X\n",
    "            x = list()\n",
    "            # The words in lower case\n",
    "            for j in range(2 * self.w_size + 1):\n",
    "                if self.tolower:\n",
    "                    x.append(sentence[i + j][self.input].lower())\n",
    "                else:\n",
    "                    x.append(sentence[i + j][self.input])\n",
    "            # We represent the feature vector as a dictionary\n",
    "            X.append(dict(zip(self.feature_names, x)))\n",
    "        return X, y\n",
    "\n",
    "    def print_example(self, sentences, id=1968):\n",
    "        \"\"\"\n",
    "        :param corpus:\n",
    "        :param id:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # We print the features to check they match Table 8.1 in my book (second edition)\n",
    "        # We use the training step extraction with the dynamic features\n",
    "        Xs, ys = self._transform_sentence(sentences[id])\n",
    "        print('X for sentence #', id, Xs)\n",
    "        print('y for sentence #', id, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_dictorizer = ContextDictorizer()\n",
    "context_dictorizer.fit(train_dict)\n",
    "X_dict, y_cat = context_dictorizer.transform(train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_dictorizer.print_example(train_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We extract all the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_words = []\n",
    "for x in X_dict:\n",
    "    corpus_words.extend(list(x.values()))\n",
    "corpus_words = sorted(list(set(corpus_words)))\n",
    "print('# unique words seen in training corpus:', len(corpus_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We add these words to the vocabulary\n",
    "We add one word to the count for the unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_words = embeddings_dict.keys()\n",
    "print('Words in GloVe:',  len(embeddings_dict.keys()))\n",
    "vocabulary_words = set(corpus_words + list(embeddings_words))\n",
    "cnt_uniq = len(vocabulary_words) + 1\n",
    "print('# unique words in the vocabulary: embeddings and corpus:', \n",
    "      cnt_uniq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now build an index\n",
    "We keep index 0 for the unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_rev_idx = dict(enumerate(vocabulary_words, start=1))\n",
    "word_idx = {v: k for k, v in word_rev_idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We replace the words with their index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x_dict in X_dict:\n",
    "    for word in x_dict:\n",
    "        x_dict[word] = word_idx[x_dict[word]]\n",
    "dict_vect = DictVectorizer()\n",
    "X = dict_vect.fit_transform(X_dict)\n",
    "\n",
    "print('X shape', X.shape)\n",
    "print('First line of X:', X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And $\\mathbf{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The POS and the number of different POS\n",
    "pos_list = sorted(list(set(y_cat)))\n",
    "NB_CLASSES = len(pos_list) + 1\n",
    "\n",
    "# We build a part-of-speech index. We keep 0 for unknown symbols in the test set\n",
    "pos_rev_idx = dict(enumerate(pos_list, start=1))\n",
    "pos_idx = {v: k for k, v in pos_rev_idx.items()}\n",
    "\n",
    "# We encode y\n",
    "y = [pos_idx[i] for i in y_cat]\n",
    "print(y_cat[:10])\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now create the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.random((len(vocabulary_words) + 1, \n",
    "                                     EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in vocabulary_words:\n",
    "    if word in embeddings_dict:\n",
    "        # If the words are in the embeddings, we fill them with a value\n",
    "        embedding_matrix[word_idx[word]] = embeddings_dict[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(cnt_uniq, EMBEDDING_DIM, \n",
    "                                input_length=2 * W_SIZE + 1))\n",
    "if embedding_matrix is not None:\n",
    "    model.layers[0].set_weights([embedding_matrix])\n",
    "    model.layers[0].trainable = False\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(NB_CLASSES, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "                   optimizer=OPTIMIZER,\n",
    "                   metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(X, y, epochs=EPOCHS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_dict, y_test_cat = context_dictorizer.transform(test_dict)\n",
    "for x_dict_test in X_test_dict:\n",
    "    for word in x_dict_test:\n",
    "        x_dict_test[word] = word_idx.get(x_dict_test[word], 0)\n",
    "            \n",
    "# We transform the symbols into numbers\n",
    "X_test = dict_vect.transform(X_test_dict)\n",
    "y_test = [pos_idx.get(i, 0) for i in y_test_cat]\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Optimizer', OPTIMIZER, 'Epochs', EPOCHS, 'Batch size', \n",
    "      BATCH_SIZE, 'Mini corpus', MINI_CORPUS)\n",
    "print('Loss:', test_loss)\n",
    "print('Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of parts of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence(sentence, dict_vect, model, word_idx, pos_rev_idx):\n",
    "    column_names = ['id', 'form']\n",
    "    sentence = list(enumerate(sentence.lower().split(), start=1))\n",
    "    conll_cols = ''\n",
    "    for tuple in sentence:\n",
    "        conll_cols += str(tuple[0]) + '\\t' + tuple[1] + '\\n'\n",
    "    # print(conll_cols)\n",
    "\n",
    "    conll_dict = CoNLLDictorizer(column_names, col_sep='\\t')\n",
    "    sent_dict = conll_dict.transform(conll_cols)\n",
    "    # print('Sentence:', sent_dict[0])\n",
    "\n",
    "    context_dictorizer = ContextDictorizer()\n",
    "    context_dictorizer.fit(sent_dict)\n",
    "    X_dict, y = context_dictorizer.transform(sent_dict, \n",
    "                                             training_step=False)\n",
    "    # print('Sentence, padded:', X_dict)\n",
    "    # print('POS, y:', y)\n",
    "\n",
    "    for x_dict in X_dict:\n",
    "        for word in x_dict:\n",
    "            x_dict[word] = word_idx.get(x_dict[word], 1)\n",
    "    X = dict_vect.transform(X_dict)\n",
    "\n",
    "    # print(X)\n",
    "    y_prob = model.predict(X)\n",
    "    y_pred = y_prob.argmax(axis=-1)\n",
    "    y_pred_cat = [pos_rev_idx[i] for i in y_pred]\n",
    "    return y_pred_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"That round table might collapse .\",\n",
    "                 \"The man can learn well .\",\n",
    "                 \"The man can swim .\",\n",
    "                 \"The man can simwo .\"]\n",
    "for sentence in sentences:\n",
    "    y_test_pred_cat = predict_sentence(sentence.lower(), \n",
    "                                       dict_vect,\n",
    "                                       model, word_idx, \n",
    "                                       pos_rev_idx)\n",
    "    print(sentence)\n",
    "    print(y_test_pred_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
