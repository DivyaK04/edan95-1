{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Generation\n",
    "Generating sentences with conditional probabilities\n",
    "\n",
    "Author: Pierre Nugues, modified by Marcus Klang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(os.environ[\"SPARK_HOME\"], \"python\"))\n",
    "sys.path.append(os.path.join(os.environ[\"SPARK_HOME\"], \"python\", \"lib\", \"py4j-0.10.7-src.zip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = (pyspark.SparkConf()\n",
    "                .setAll([('spark.executor.memory', '8g'), \n",
    "                         ('spark.driver.memory','8g')]))\n",
    "\n",
    "sc = pyspark.SparkContext(conf=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENWIKI_FULL=\"file:/usr/local/cs/EDAN95/datasets/wikipedia/enwiki\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a Corpus\n",
    "Utility function to read all the files in a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#def get_files(dir, suffix):\n",
    "#    \"\"\"\n",
    "#    Returns all the files in a folder ending with suffix\n",
    "#    :param dir:\n",
    "#    :param suffix:\n",
    "#    :return: the list of file names\n",
    "#    \"\"\"\n",
    "#    files = []\n",
    "#    for file in os.listdir(dir):\n",
    "#        if file.endswith(suffix):\n",
    "#            files.append(file)\n",
    "#    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "An elemetary tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def tokenize(text):\n",
    "#    \"\"\"\n",
    "#    Uses the letters to break the text into words.\n",
    "#    Returns a list of match objects\n",
    "#    \"\"\"\n",
    "#    words = re.findall('\\p{L}+', text)\n",
    "#    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Files\n",
    "We read a corpus of novels from Dickens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder = '/Users/pierre/Documents/Cours/EDAN20/corpus/Selma/'\n",
    "#folder = '/Users/pierre/Documents/Cours/EDAN20/corpus/Dickens/'\n",
    "#files = get_files(folder, 'txt')\n",
    "#files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tokenize the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words = []\n",
    "#for file in files:\n",
    "#    text = open(folder + file).read().lower().strip()\n",
    "#    words += tokenize(text)\n",
    "#words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = sc.textFile(ENWIKI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize and preprocess the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = re.compile(\"\\p{L}+\")\n",
    "word_bags = (\n",
    "    wiki\n",
    "    .map(str.lower)\n",
    "    .map(lambda ln: [tok for tok in ln.split() if m.fullmatch(tok) is not None])\n",
    "    .filter(lambda bag: len(bag) > 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cutoff setting for reducing final memory requirements for bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUTOFF=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create word mapping for memory and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = word_bags.flatMap(lambda x: x).distinct().sortBy(lambda x:x).zipWithIndex().collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2785336"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2id[\"lund\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = {v:k for k,v in word2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Share it for parallel use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id_map = sc.broadcast(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def count_unigrams(words):\n",
    "#    frequency = {}\n",
    "#    for word in words:\n",
    "#        if word in frequency:\n",
    "#            frequency[word] += 1\n",
    "#       else:\n",
    "#           frequency[word] = 1\n",
    "#    return frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "unigram_freq = (\n",
    "    word_bags\n",
    "    .flatMap(lambda x: [(word2id_map.value[w],1) for w in x])\n",
    "    .reduceByKey(add)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def count_bigrams(words):\n",
    "#    bigrams = [tuple(words[idx:idx + 2])\n",
    "#               for idx in range(len(words) - 1)]\n",
    "#    frequencies = {}\n",
    "#    for bigram in bigrams:\n",
    "#        if bigram in frequencies:\n",
    "#            frequencies[bigram] += 1\n",
    "#        else:\n",
    "#           frequencies[bigram] = 1\n",
    "#    return frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_freq = (\n",
    "    word_bags\n",
    "    .flatMap(lambda words: [ (tuple(map(word2id_map.value.__getitem__, words[idx:idx + 2])),1) for idx in range(len(words) - 1)])\n",
    "    .reduceByKey(add)\n",
    "    .filter(lambda tup: tup[1] > CUTOFF) # Huge result otherwise!\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We count the unigrams and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = unigram_freq.collectAsMap() #count_unigrams(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244250"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigrams[word2id['master']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigrams = bigram_freq.collectAsMap() # count_bigrams(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given a bigram, $w_n, w_{n+1}$, we compute $P(w_{n+1}|w_n)$. This is defined as $\\frac{count(w_n, w_{n+1})}{count(w_n)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_bc = sc.broadcast(unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.storagelevel import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_conditionals(items):\n",
    "    word2prob = sorted(items, key=lambda tup: tup[1], reverse=True)\n",
    "    prs = np.array([pr for w,pr in word2prob], dtype=np.float32)\n",
    "    prs /= np.sum(prs) # Rebalance so that the sum always equals to 1, due to cutoff.\n",
    "    return np.array([w for w,pr in word2prob]), prs\n",
    "\n",
    "cond_prop_sorted_rdd = (\n",
    "    bigram_freq\n",
    "    .map(lambda tup: (tup[0][0], (tup[0][1], tup[1]/unigrams_bc.value[tup[0][0]])))\n",
    "    .groupByKey()\n",
    "    .mapValues(map_conditionals)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_prop_sorted = cond_prop_sorted_rdd.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5005660,  178902, 2116433, 5202899, 4632207, 4773250, 2170440,\n",
       "         770169,  593128, 1922322, 5240452, 4831101,       0, 1905812,\n",
       "        3458113,  276751, 2039194, 1862630,  139372, 1598748, 3170331,\n",
       "        5237264, 4627294,  446772, 3429824, 2183913, 2828566, 5262971,\n",
       "        4327786, 1567952,  298426, 2428972, 1176781, 5237650, 2149877,\n",
       "         688606,   66848, 2825216,  682422, 5227302, 1557332, 2944551,\n",
       "        4424331, 2143616, 2785336,  948000, 2259817, 5274714, 3416079,\n",
       "         171036, 3323165, 4297293, 4846096, 5013391, 4378498, 1944930,\n",
       "        4527463, 2653323,  452205, 3723253, 4789557, 3982844, 1891472,\n",
       "        1980611, 4777335, 2674011, 5017911, 1502161, 4391587, 4150697,\n",
       "        4547992, 2022398, 5282036, 3140212, 1283118, 1483864,  451911,\n",
       "        3780699,  169634, 4780067, 4290262, 5203684, 4783615, 5237171,\n",
       "        2866739,  229420, 1175457,  248090,  883098, 3584278, 4568794,\n",
       "        3953278, 2638357, 2276846, 1260653, 1258130,  770661,  226276,\n",
       "        4219435, 4023886, 1248328, 1434335, 5226348, 5286053, 4528480,\n",
       "        3750040, 3422367, 4261468, 3373407, 1537779, 2275535,  599437,\n",
       "        2217865,  907390, 4425132, 1412370, 2478711, 4856455,  491942,\n",
       "        4771739, 3476871, 2045251, 4237695, 2259049, 1780890, 4569243,\n",
       "        1609091, 1578182, 3854749, 1947499,  924808, 1106420, 1915218,\n",
       "        1597255, 1571619,  180286, 1092962, 1771692, 4861064, 1457636]),\n",
       " array([0.20751275, 0.10703866, 0.07695113, 0.06181619, 0.05342815,\n",
       "        0.02844639, 0.02607586, 0.0195113 , 0.01714077, 0.01695842,\n",
       "        0.01604668, 0.0131291 , 0.01294675, 0.01167031, 0.01039387,\n",
       "        0.01002918, 0.00911743, 0.00838804, 0.00784099, 0.00765864,\n",
       "        0.0071116 , 0.0071116 , 0.0067469 , 0.0063822 , 0.0063822 ,\n",
       "        0.00565281, 0.00565281, 0.00565281, 0.00492341, 0.00474106,\n",
       "        0.00474106, 0.00419402, 0.00419402, 0.00401167, 0.00401167,\n",
       "        0.00401167, 0.00382932, 0.00382932, 0.00364697, 0.00364697,\n",
       "        0.00346462, 0.00328228, 0.00309993, 0.00309993, 0.00309993,\n",
       "        0.00309993, 0.00309993, 0.00291758, 0.00291758, 0.00291758,\n",
       "        0.00291758, 0.00273523, 0.00273523, 0.00273523, 0.00273523,\n",
       "        0.00273523, 0.00255288, 0.00255288, 0.00237053, 0.00237053,\n",
       "        0.00218818, 0.00218818, 0.00218818, 0.00218818, 0.00218818,\n",
       "        0.00218818, 0.00218818, 0.00200584, 0.00200584, 0.00200584,\n",
       "        0.00200584, 0.00200584, 0.00200584, 0.00182349, 0.00182349,\n",
       "        0.00182349, 0.00182349, 0.00182349, 0.00182349, 0.00182349,\n",
       "        0.00182349, 0.00182349, 0.00182349, 0.00182349, 0.00164114,\n",
       "        0.00164114, 0.00164114, 0.00164114, 0.00164114, 0.00164114,\n",
       "        0.00164114, 0.00164114, 0.00164114, 0.00164114, 0.00145879,\n",
       "        0.00145879, 0.00145879, 0.00145879, 0.00145879, 0.00145879,\n",
       "        0.00145879, 0.00145879, 0.00145879, 0.00145879, 0.00127644,\n",
       "        0.00127644, 0.00127644, 0.00127644, 0.00127644, 0.00127644,\n",
       "        0.00127644, 0.00127644, 0.00127644, 0.00127644, 0.00127644,\n",
       "        0.00127644, 0.00109409, 0.00109409, 0.00109409, 0.00109409,\n",
       "        0.00109409, 0.00109409, 0.00109409, 0.00109409, 0.00109409,\n",
       "        0.00109409, 0.00109409, 0.00109409, 0.00109409, 0.00109409,\n",
       "        0.00109409, 0.00109409, 0.00109409, 0.00109409, 0.00109409,\n",
       "        0.00109409, 0.00109409, 0.00109409, 0.00109409, 0.00109409],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cond_prop_sorted[word2id[\"lund\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "master of              = 0.27679145336151123\n",
      "master degree          = 0.14825420081615448\n",
      "master and             = 0.04531529173254967\n",
      "master in              = 0.03585680574178696\n",
      "master plan            = 0.025811992585659027\n",
      "master degrees         = 0.02226676233112812\n",
      "master the             = 0.013803665526211262\n",
      "master at              = 0.012767367996275425\n",
      "master classes         = 0.011135654523968697\n",
      "master was             = 0.008794893510639668\n",
      "master is              = 0.0068586538545787334\n",
      "master thesis          = 0.006463224533945322\n",
      "master to              = 0.006272327620536089\n",
      "master he              = 0.0056860013864934444\n",
      "master who             = 0.0056632752530276775\n",
      "master builder         = 0.005586007609963417\n",
      "master system          = 0.005331478547304869\n",
      "master sergeant        = 0.005290571600198746\n",
      "master class           = 0.004849690478295088\n",
      "master for             = 0.004822419956326485\n",
      "master chief           = 0.004676974844187498\n",
      "master tapes           = 0.004454261623322964\n",
      "master program         = 0.004431535489857197\n",
      "master from            = 0.00425881939008832\n",
      "master or              = 0.004254274535924196\n",
      "master mason           = 0.0037270353641361\n",
      "master control         = 0.003717944724485278\n",
      "master general         = 0.0036815835628658533\n",
      "master level           = 0.003604315686970949\n",
      "master as              = 0.003477051155641675\n",
      "master planned         = 0.003349786391481757\n",
      "master a               = 0.003340696217492223\n",
      "master programs        = 0.0030179894529283047\n",
      "master on              = 0.0028634537011384964\n",
      "master p               = 0.0025316569954156876\n",
      "master title           = 0.002468024380505085\n",
      "master with            = 0.002418027725070715\n",
      "master bedroom         = 0.002349850255995989\n",
      "master had             = 0.0023043984547257423\n",
      "master house           = 0.0022725823801010847\n",
      "master voice           = 0.0022134953178465366\n",
      "master craftsman       = 0.002045324072241783\n",
      "master but             = 0.0020316888112574816\n",
      "master which           = 0.0020316888112574816\n",
      "master by              = 0.0019907820969820023\n",
      "master it              = 0.0018089755903929472\n",
      "master carpenter       = 0.00174534332472831\n",
      "master race            = 0.001740798121318221\n",
      "master tape            = 0.0017180723370984197\n",
      "master recordings      = 0.0016408045776188374\n"
     ]
    }
   ],
   "source": [
    "word, probs = cond_prop_sorted[word2id[\"master\"]]\n",
    "for w, p in list(zip(word, probs))[0:50]:\n",
    "    print('master', \"{0: <15} = {1}\".format(id2word[w], p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probs = {k: v/unigrams[k[0]] for k, v in bigrams.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cond_prob = sorted([(k, v) for k, v in probs.items() if k[0] == 'master'],\n",
    "#                    key=lambda tup: tup[1], reverse=True)\n",
    "#cond_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawing samples from using a distribution. Understanding the `np.random.choice` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3055\n",
      "1 4973\n",
      "2 1972\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "outputs = np.random.choice(np.arange(0,3), size=10000, p=[0.3, 0.5, 0.2])\n",
    "print(\"0\", np.sum(outputs == 0))\n",
    "print(\"1\", np.sum(outputs == 1))\n",
    "print(\"2\", np.sum(outputs == 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the conditional probabilities of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def cond_prob(word):\n",
    "#    cprob = sorted([(k, v) for k, v in probs.items() if k[0] == word],\n",
    "#                    key=lambda tup: tup[1], reverse=True)\n",
    "#    return cprob\n",
    "#cond_prob('master')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save all important parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lm.pkl\", \"wb\") as fout:\n",
    "    pickle.dump({\n",
    "        \"word2id\": word2id,\n",
    "        \"id2word\": id2word,\n",
    "        \"cond_prop_sorted\": cond_prop_sorted\n",
    "    }, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And finally, generating a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperature(p, temperature):\n",
    "    preds = np.log(p) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skåne county pennsylvania public schools feed on april was not in american numbering was founded by a programmer who was added to date from wjla began writing development and eighteen more to switch to a four public library in the inspiring moments were as part of land in the top south coast of india he was generally use the village is the southwest of the annual gross came from to land bonded to a mechanic lien is a portion of parma which occurred only instance if the ancestor the same test scores in february his narrow gauge track and blow at "
     ]
    }
   ],
   "source": [
    "TEMPERATURE=1.0\n",
    "word = word2id['skåne']\n",
    "print(id2word[word], end=' ')\n",
    "for i in range(100):\n",
    "    if word not in cond_prop_sorted:\n",
    "        word = word2id[\"the\"] # For bigrams for which the second words does not have a continuation (too rare)\n",
    "        \n",
    "    words, distribution = cond_prop_sorted[word]\n",
    "    #distribution = [i[1] for i in cprob]\n",
    "    word = np.random.choice(words, size=1, p=temperature(distribution, TEMPERATURE))[0]\n",
    "    print(id2word[word], end=' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
